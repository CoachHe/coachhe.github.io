# 构建一个典型的 Flink 流式应用的步骤

1. 设置执行环境
2. 从数据源读取一条或多条流
3. 通过一系列流式转换来实现应用逻辑
4. 选择性地将结果输出到一个或多个数据汇中
5. 执行程序

![](https://coachhe.oss-cn-shenzhen.aliyuncs.com/Scala/20210223201933.png)

这是一个 flink 程序的基本流程。

# 一、 Environment

1. getExecutionEnvironment

   创建一个执行环境，表示当前执行程序的上下文。

   如果程序是**独立调用**的，则此方法返回本地执行环境；

   如果从**命令行客户端调用程序以提交到集群**，则此方法返回此集群的执行环境

   也就是说: getExecutionEnvironment 会根据查询运行的方式决定返回什么样的运行环境，是最常用的一种创建执行环境的方式。

2. createLocalEnvironment

   返回集群执行环境， 将 Jar 提交到远程服务器。 需要在调用时指定 JobManager 的 IP 和端口号， 并指定要在集群中运行的 Jar 包。

   ```java
   val env = StreamExecutionEnvironment.createLocalEnvironment(1)
   ```

3. createRemoteEnvironment

   返回集群执行环境， 将 Jar 提交到远程服务器。 需要在调用时指定 JobManager 的 IP 和端口号， 并指定要在集群中运行的 Jar 包。

   ```scala
   val env = ExecutionEnvironment.createRemoteEnvironment("jobmanage-hostname", 6123,"YOURPATH//wordcount.jar")
   ```

# 二、Source

### 读取数据的方式

共有四种

1. 从集合中读取数据
2. 从文本文件中读取数据
3. 从消息队列（kafka）中读取数据
4. 自定义 Source

### 1. 从集合中读取数据

```scala
val dataList = List(
    SensorReading("sensor_1", 1547718199, 35.8),
    SensorReading("sensor_6", 1547718201, 15.4),
    SensorReading("sensor_7", 1547718202, 6.7),
    SensorReading("sensor_10", 1547718205, 38.1)
)
val stream1 = env.fromCollection(dataList)
stream1.print()
```

### 2. 从文本文件中读取数据

```scala
val inputPath = "/Users/heyizhi/Nutstore Files/我的坚果云/程序员/学习/大数据/10 flink/1st flink project/flinkTutorial/src/main/resources/sensor.txt"
val stream2 = env.readTextFile(inputPath)
stream2.print()
```

### 3. 从 Kafka 中读取数据

① 启动 kafka

![](https://coachhe.oss-cn-shenzhen.aliyuncs.com/Scala/20210223233317.png)

② 创建 topic

![](https://coachhe.oss-cn-shenzhen.aliyuncs.com/Scala/20210223233439.png)

③ 读取数据

```scala
val properties = new Properties()
properties.setProperty("bootstrap.servers", "localhost:9092")
properties.setProperty("group.id", "consumer-group")
val stream3 = env.addSource(new FlinkKafkaConsumer011[String]("sensor", new SimpleStringSchema(), properties))
stream3.print()
```

### 4. 自定义 Source

需要自定义 SourceFunction[SensorReading]

![](https://coachhe.oss-cn-shenzhen.aliyuncs.com/Scala/20210224103647.png)

Source 类

需要继承 SourceFunction

![](https://coachhe.oss-cn-shenzhen.aliyuncs.com/Scala/20210224103931.png)

# 三、 Transform

流式转换以一个或多个数据流为输入，并将它们转换成一个或多个输出流。

## DataStream API 程序的本质

通过组合不同的转换来创建一个满足应用逻辑的 Dataflow 图。

## DataStream API 算子种类

这里的算子主要分为四类

1. 作用于单个事件的基本转换（例如 map）
2. 针对相同键值事件的 KeyedStream 转换
3. 将多条数据流合并为一条或将一条数据流拆分成多条流的转换
4. 对流中的事件进行重新组织的分发转换

## 简单转换算子

map、flatMap 和 Filter 称为简单转换算子

### 1. map

一对一转换

<img src="https://coachhe.oss-cn-shenzhen.aliyuncs.com/Scala/20210224160034.png" style="zoom:33%;" />

#### 语法

```scala
val streamMap = stream.map{x => x * 2}
```

### 2. flatMap

相当于是把集合类型的元素打散，从一个集合（例如二元数组）转换后再将结果拼接成一个打散的集合（例如一元数组）

flatMap 的**函数签名**：

```scala
def flatMap[A,B](as: List[A])(f: A = List[B]):List[B]
```

例如:

```scala
flatMap(List(1,2,3))(i ⇒ List(i,i))
```

结果是 `List(1,1,2,2,3,3)`,

而

```scala
List(" a b" , "c d" ).flatMap(line ⇒ line.split(" "))
```

结果是 `List(a, b, c, d)`。

#### 语法

```scala
val streamFlatMap = stream.flatMap{
  x => x.split(" ")
}
```

### 3. Filter

过滤，对每个元素进行判断

<img src="https://coachhe.oss-cn-shenzhen.aliyuncs.com/Scala/20210224161505.png" style="zoom:33%;" />

#### 语法

```scala
val streamFilter = stream.filter{
  x => x == 1
}
```

## 键控流转换算子（基于 KeyedStream 的转换）

<img src="https://coachhe.oss-cn-shenzhen.aliyuncs.com/Scala/20210224161649.png" style="zoom:33%;" />

DataStream $\rightarrow$ KeyedStream，逻辑地将一个流拆分成不想交的分区，每个分区包含具有相同 key 的元素，在内部以 hash 的形式实现。

#### 注意

在这里 KeyedStream 继承了 DataStream，所以其本质上还是一个 DataStream。

### 滚动聚合算子

这些算子可以针对 KeyedStream 的每一个支流做聚合

- `sum()`

  滚动计算输入流中指定字段的和

- `min()`

  滚动计算输入流中指定字段的最小值

- `max()`

  滚动计算输入流中指定字段的最大值

- `minBy()`

  滚动计算输入流中迄今为止最小值，返回该值所在事件

- `maxBy()`

  滚动计算输入流中迄今为止最大值，返回该值所在事件

#### 注意

无法将多个滚动聚合方法组合使用，每次只能计算一个

#### 示例

传感器示例中以 id 分区，求当前温度最小值

```scala
package com.coachhe.apitest

object TransformTest {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    // 读取数据
    val inputPath = "/Users/heyizhi/Nutstore Files/我的坚果云/程序员/学习/大数据/10 flink/1st flink project/flinkTutorial/src/main/resources/sensor.txt"
    val inputStream = env.readTextFile(inputPath)

    // 转换成样例类
    val dataStream = inputStream
      .map(data => {
        val arr = data.split(",")
        SensorReading(arr(0), arr(1).toLong, arr(2).toDouble)
      })

    // 分组聚合，输出每个传感器当前温度最小值
    val aggStream = dataStream
      .keyBy(0)
      .min("temperature")
    aggStream.print()

    env.execute("transform test")
  }
}
```

### Reduce

`reduce` 转换是滚动聚合转换的**泛化**。

它将 `ReduceFunction` 应用在一个 `KeyedStream` 上，每个到来时间都会和 `reduce` 结果进行一次组合，从而产生一个新的`DataStream`。`reduce` 转换不会改变数据类型，因此输出流的类型会永远和输入流保持一致。

#### reduce 方法源码

```java
@FunctionalInterface
@Public
public interface ReduceFunction<T> extends Function, Serializable {
    T reduce(T var1, T var2) throws Exception;
}
```

可以看到，这里的 `reduce` 方法的输入参数是 两个 `T` 类型的变量，最终输出也是一个 `T` 类型的变量，然后这个输出的变量会和下一个参数继续进行 `reduce` 操作，直到最后执行完成，输出最后一次 `reduce` 的结果。

#### 示例

1. 传感器示例中以 id 分区，求当前温度最小值，要求时间戳为当前时间戳

```scala
        // 需要输出当前最小的温度值，以及最近的时间戳，要用reduce
        val resultStream = dataStream
          .keyBy("id")
          .reduce((curState, newData) =>
            SensorReading(curState.id, newData.timestamp, curState.temperature.min(newData.temperature))
          )
```

2. 获取每个用户的访问频次

```java
        DataStreamSource<Event> stream = env.fromElements(new Event("Mary", "./home", 1000L),
                new Event("Bob", "./cart", 2000L),
                new Event("Alice", "./prod?id=100", 3000L),
                new Event("Bob", "./prod?id=1", 3300L),
                new Event("Alice", "./prod?id=200", 3200L),
                new Event("Bob", "./home", 3500L),
                new Event("Bob", "./prod?id=2", 3800L),
                new Event("Bob", "./prod?id=3", 4200L)
        );

        // 1.统计每个用户的访问频次
        SingleOutputStreamOperator<Tuple2<String, Long>> clickByUser = stream
                // 首先使用map方法将stream中的event挨个转换成Tuple2<String, Long>的形式,其中第一个元素是user，第二个元素固定是1L
                .map((MapFunction<Event, Tuple2<String, Long>>) event -> Tuple2.of(event.user, 1L))
                // 然后将Tuple2<String, Long>类型的所有data根据其f0元素（也就是Tuple2的第一个元素event.user）分组，获得一个KeyedStream
                .keyBy(data -> data.f0)
                // 最后将keyStream进行reduce操作，对每两个元素(Tuple2<String, Long>类型)执行函数，最终获得Tuple2类型的一个结果,第一个元素是user，第二个元素是1L之和，其实就是有多少个点击次数
                .reduce((ReduceFunction<Tuple2<String, Long>>) (value1, value2) -> Tuple2.of(value1.f0, value1.f1 + value2.f1)
                );

        // 2. 获取当前最活跃的用户
        SingleOutputStreamOperator<Tuple2<String, Long>> result = clickByUser
                // 首先根据对应的key（也就是用户）进行分组
                .keyBy(data -> "key")
                // 然后比较每个值的f1，也就是点击量之和，最终输出最大值
                .reduce((ReduceFunction<Tuple2<String, Long>>) (value1, value2) -> value1.f1 > value2.f1 ? value1 : value2);

```

### 用户自定义函数（UDF）

#### 函数类

##### Java 实现

Flink 中所有用户自定义函数(如 MapFunction, FilterFunction 及 ProcessFunction)的接口都是以接口或抽象类的形式对外暴露。

我们可以通过实现接口或继承抽象类的方式实现函数。下面的例子实现了一个 FilterFunction,用来过滤出所有包含“flink"― 词的字符串。

```java
    public static class FlinkFilter implements FilterFunction<Event> {
        @Override
        public boolean filter(Event value) throws Exception {
            return value.url.contains("home");
        }
    }
```

然后可以直接将函数类的实例作为参数传递给 filter 转换：

```java
        // 1. 传入实现FilterFunction接口的自定义函数类
        DataStream<Event> stream1 = clicks.filter(new FlinkFilter());
```

还可以通过匿名类来实现函数

```java
        // 2. 传入匿名类
        DataStream<Event> stream3 = clicks.filter(new FilterFunction<Event>() {
            @Override
            public boolean filter(Event value) throws Exception {
                return value.url.contains("home");
            }
        });
```

可以看到，传入了一个匿名类，内部实现了 filter 方法。

##### scala 实现

Flink 暴露了所有 udf 函数的接口（实现方式为接口或者抽象类）。例如 MapFunction，FilterFunction 和 ProcessFunction 等。

几乎所有的算子都有对应的 udf 函数类，也就是可以定制。

例如定义一个 filter，如下如梭

```scala
// 自定义一个函数类
class MyFilter extends FilterFunction[SensorReading] {
  override def filter(t: SensorReading): Boolean =
    t.id.startsWith("sensor1")
}
```

在继承一个 FilterFunction 之后继承了 filter 方法，然后可以直接使用。

```scala
val MyFilterStream = stream.filter(new MyFilter)
```

注意，这里我们定义的 Filter 和原来的流的类型一定要相同。

也可以通过匿名类来实现函数

```scala
val MyFilterStream2 = stream.filter(
	new RichFilterFunction[String]{
    override def filter(value: String): Boolean = {
      value.contains("flink")
    }
  }
)
```

此外，函数可以通过其构造函数接收参数。

```scala
val tweets: DataStream[String] = ???
val flinkTweets = tweets.filter(new KeywordFilter("flink"))

// 自定义一个函数类
class MyFilter(keyWord: String) extends FilterFunction[SensorReading] {
  override def filter(t: SensorReading): Boolean =
    t.id.contains(keyWord)
}
```

在例子中，我们通过构造函数传入了“flink”参数，可以作为参数使用

#### Lambda 函数

##### scala 实现

```java
        // 3. 传入Lambda表达式
        SingleOutputStreamOperator<Event> stream4 = clicks.filter(data -> data.url.contains("home"));
```

##### java 实现

#### 富函数

很多时候，我们需要在函数处理第一条记录之前进行一些初始化工作或是取得函数执行相关的上下文信息。Datastream API 提供了一类富函数，它和我们之前见到的普通函数相比可对外提供更多功能。

DataStream API 中所有的转换函数都有对应的富函数。富函数的使用位置和普通函数以及 Lambda 函数相同。它们可以像普通函数类一样接收参数。 富函数的命名规则是以 Rich 开头，后面跟着普通转换函数的名字，例如： RichMapFunction、 RichFlatMapFunction 等。

在使用富函数时，可以对应函数的生命周期实现两个额外的方法：

- open()方法是富函数中的初始化方法。它在每个任务首次调用转换方法（如 filter 或 map）前调用一次。 open()通常用于那些只需进行一次的设置工作。
- close()作为函数的终止方法，会在每个任务最后一次调用转换方法后调用一次。它通常用于清理和释放资源

此外，可以利用 getRuntimeContext()方法访问函数的 RuntimeContext。从 RuntimeContext 中能获取到一些信息，例如函数的并行度，函数所在子任务的编号以及执行函数的任务名称。同时，它还提供访问分区状态的方法。

注意：
富函数不能使用 lambda 表达式，因为这里不是只有一个抽象方法。所以只能用函数类

##### java 实现

```java
        // 将点击事件转换成长整型的时间戳输出
        clicks.map(new RichMapFunction<Event, Long>() {
            		// 这里替换了父类的open()方法，会在map方法执行前被执行一次
                    @Override
                    public void open(Configuration parameters) throws Exception {
                        super.open(parameters);
                        System.out.println("索引为 " + getRuntimeContext().getIndexOfThisSubtask() + " 的任务开始");
                    }

            		// 必须实现的一个方法
                    @Override
                    public Long map(Event value) throws Exception {
                        return value.timestamp;
                    }

            		// 这里替换了父类的close()方法，会在所有map方法执行后被执行一次
                    @Override
                    public void close() throws Exception {
                        super.close();
                        System.out.println("索引为 " + getRuntimeContext().getIndexOfThisSubtask() + " 的任务结束");
                    }
                })
                .print();

```

##### scala 实现

```scala
        // 富函数
        class MyRichFilter extends RichFilterFunction[SensorReading] {
          override def filter(t: SensorReading): Boolean = ???

          override def open(parameters: Configuration): Unit = {
            //做一些初始化操作，比如数据库的连接
          }

          override def close(): Unit = {
            // 一般做收尾工作，比如关闭连接，或者清空状态
          }
        }
```

## 分流操作（split 和 select）

split 转换是 union 转换的逆操作。它将输入流分割成两条或多条类型和输入流相同的输出流。每一个到来的事件都可以被发往零个、一个或多个输出流。
因此，split 也可以用来过滤或复制事件。

<img src="https://coachhe.oss-cn-shenzhen.aliyuncs.com/Scala/20210224181644.png" style="zoom:33%;" />
上图中，split算子将所有白色事件和其他事件分开，发往不同的数据流

具体来说

### split

<img src="https://coachhe.oss-cn-shenzhen.aliyuncs.com/Scala/20210224191817.png" style="zoom:33%;" />

DataStream → SplitStream：根据某些特征把一个 DataStream 拆分成两个或者多个 DataStream。

### select

<img src="https://coachhe.oss-cn-shenzhen.aliyuncs.com/Scala/20210224191934.png" style="zoom:33%;" />

split 和 select 总是一起出现，**select()方法会在每个输入时间到来时被调用，并随机返回一个对象。类似 KeyBy()方法之后总是跟着一个滚动聚合算子**。split 函数就会根据这个 select 方法确认应该将上游的数据分发到哪些输出流。

### 示例

将传感器数据通过温度分开，以 30 度为界

```scala
    // 4.分流操作
    val splitStream = dataStream
      .split(
        data => {
          if (data.temperature > 30.0) Seq("high") else Seq("low")
        }
      )
    val highTempStream: DataStream[SensorReading] = splitStream.select("high")
    val lowTempStream: DataStream[SensorReading] = splitStream.select("low")
    val allTempStream: DataStream[SensorReading] = splitStream.select("high", "low")

    highTempStream.print("high")
    lowTempStream.print("low")
    allTempStream.print("all")
```

## 合流操作

### connect

<img src="https://coachhe.oss-cn-shenzhen.aliyuncs.com/Scala/20210224223544.png" style="zoom:33%;" />

DataStream,DataStream → ConnectedStreams：连接两个保持他们类型的数据流， 两个数据流被 Connect 之后， 只是被放在了一个同一个流中， 内部依然保持各自的数据和形式不发生任何变化， 两个流相互独立。

### CoMap,CoFlatMap

<img src="https://coachhe.oss-cn-shenzhen.aliyuncs.com/Scala/20210224223711.png" style="zoom:33%;" />

ConnectedStreams → DataStream：作用于 ConnectedStreams 上， 功能与 map 和 flatMap 一样， 对 ConnectedStreams 中的每一个 Stream 分别进行 map 和 flatMap 处理。

```scala
    // 4.2 合流操作
    val warningStream = highTempStream
      .map(data => (data.id, data.temperature))

    val  connectedStreams = warningStream.connect(lowTempStream)

    // 用coMap对数据进行分别处理
    val coMapResultStream = connectedStreams
      .map(
        warningData => (warningData._1, warningData._2, "warning"),
        lowTempData => (lowTempData.id, "healthy")
      )

    coMapResultStream.print("coMap")
```

### Union

DataStream.union()方法可以合并两条或多条**类型相同**的 DataStream，生成一个**新的类型相同**的 DataStream。

<img src="https://coachhe.oss-cn-shenzhen.aliyuncs.com/Scala/20210224225326.png" style="zoom:33%;" />

## 分发转换

分发转换操作定义了如何将事件分配给不同任务。某些时候，我们有必要或希望能够在应用级别控制这些分区策略，或者自定义分区器。例如，如果我们知道 DataStream 的并行分区存在数据倾斜，那么可能就希望通过重新平衡数据来均匀分配后续算子的负载。

注意：
keyBy()和分发转换是不同的，keyBy()会生成一个 KeyedStream，是一个逻辑分区，分区后的数据并没有真正承载到不同的 slot 上，但分发转换生成的是 DataStream，是真正承载在不同 Slot 上的。

### 随机

可以利用 DataStream.shuffle()方法实现随机数据交换策略。该方法会依照均匀分布随机地将记录发往后继算子的并行任务。

### 轮流

rebalance()方法会将输入流中的事件以轮流方式均匀分配给后继任务。

### 重调

rescale()也会以轮流方式对事件进行分发，但分发目标仅限于部分后继任务。

用法：

```java
        DataStreamSource<Event> stream = env.fromElements(new Event("Mary", "./home", 1000L),
                new Event("Bob", "./cart", 2000L),
                new Event("Alice", "./prod?id=100", 3000L),
                new Event("Alice", "./prod?id=200", 3500L),
                new Event("Bob", "./prod?id=2", 2500L),
                new Event("Alice", "./prod?id=300", 3600L),
                new Event("Bob", "./home", 3000L),
                new Event("Bob", "./prod?id=1", 2300L),
                new Event("Bob", "./prod?id=3", 3300L));

        // 1. 随机分区
        stream.shuffle().print().setParallelism(4);

        // 2. 轮询分区
        stream.rebalance().print().setParallelism(4);

        // 3. rescale 重缩放分区
        stream.rescale().print().setParallelism(4);
```

**轮流和重调的区别**：

rebalance()和 rescale()的本质不同体现在生成任务连接的方式。rebalance()会在所有发送任务和接收任务之间建立通信通道（例如 2 个上游任务和 10 个下游任务，那么这两个上游任务会分别和下游的这 10 个任务建立连接，一共会有 20 个连接生成）；

而 rescale()中每个发送任务只会和下游算子的部分任务建立通道（例如 2 个上游任务和 10 个下游任务，那么第 1 个上游任务会和前 5 个下游任务建立连接，第 2 个上游任务会和后 5 个下游任务建立连接，那么一共只有 10 个连接，若刚好这 10 个下游任务就分布在 2 个 TaskManager 上，那么第 1 个上游任务对应第 1 个 TaskManager，第 2 个上游任务对应第 2 个 TaskManager，就完全避免了网络上的开销）

当上游任务越多时，节省的资源则越明显。

因此 rescale 可以看做是在一些情况下对 rebalance 的一种优化

**区别演示**：

```java
        // 演示rebalance和rescale的区别
        env.addSource(new RichParallelSourceFunction<Integer>() {
            @Override
            public void run(SourceContext<Integer> sourceContext) throws Exception {
                for (int i = 0; i < 8; i++) {
                    // 将偶数和基数分开，将奇偶数分别发送到0号和1号并行分区
                    if (i % 2 == getRuntimeContext().getIndexOfThisSubtask()) {
                        sourceContext.collect(i);
                    }
                }
            }

            @Override
            public void cancel() {

            }
        }).setParallelism(2)
                .rescale()
                .print()
                .setParallelism(4);

        env.execute();
```

可以看到，这里定义了一个并行度为 2 的 Source Function，其作用是将奇数分配给 1 号分区，偶数分配给 0 号分区。如果直接打印输出，那么结果一定是无序的：

<img src="https://coachhe-1305181419.cos.ap-guangzhou.myqcloud.com/Redis/image-20221127174529343.png" width = "50%" />

因为上游任务会随机将数据流发送给下游的输出算子进行打印。

如果用 rebalance 方式进行打印，那么结果也会是无序的，因为 rebalance 方式会将处理过的数据随机分配给下游 sink 输出。

<img src="https://coachhe-1305181419.cos.ap-guangzhou.myqcloud.com/Redis/image-20221127180736451.png" width = "50%" />

如果用 rescale 方式进行打印，那么结果就会是有序的，因为一共有 4 个 sink 算子，rescale 方式会将偶数分配到前两个算子，奇数分配到后两个算子

<img src="https://coachhe-1305181419.cos.ap-guangzhou.myqcloud.com/Redis/image-20221127181100218.png" width = "50%" />

可以看到，和我们预期是相同的。

### 广播

broadcast()方法会将输入流中的事件复制并发往所有下游算子的并行任务。那么就是每个数据在下游算子全部会输出一遍。

<img src="https://coachhe-1305181419.cos.ap-guangzhou.myqcloud.com/Redis/image-20221127181356193.png" width = "50%" />

可以看到，数据源 1 在 1-4 的 sink 都输出了一遍，数据源 3 在 1-4 的数据源也都输出了一遍。

### 全局

global()方法会将输入流中的所有事件发往下游算子的第一个并行任务。

<img src="https://coachhe-1305181419.cos.ap-guangzhou.myqcloud.com/Redis/image-20221127181540244.png" width = "50%" />

可以看到，虽然输出的并行度设置为 4，但是全局分区会将所有的时间都发往第一个 sink 输出。

### 自定义

可以利用 partitionCustom()方法自己定义分区策略

## 设置并行度

#### 定义

算子并行化任务的数目称为该算子的并行度。它决定了算子处理的并行化程度以及能够处理的数据规模

#### 默认并行度

算子的并行度可以在执行环境级别或单个算子级别进行控制。默认情况下，应用内所有算子的并行度都会被设置为应用执行环境的并行度。而环境的并行度（即所有算子的默认并行度）则会根据应用启动时所处的上下文自动初始化。

如果应用是在一个本地执行环境中运行，并行度会设置为 CPU 的线程数目。如果应用是提交到 Flink 集群运行，那么除非提交客户端明确指定，否则环境并行度将设置为集群默认并行度。

#### 获取环境默认并行度方法

![](https://coachhe.oss-cn-shenzhen.aliyuncs.com/Scala/20210224233225.png)

#### 覆盖环境默认并行度方法

注意，一旦如此，将无法通过提交客户端控制应用并行度

![](https://coachhe.oss-cn-shenzhen.aliyuncs.com/Scala/20210224233344.png)

# 四、类型

Flink DataStream 应用所处理的事件会以数据对象的形式存在。函数在调用时会传入数据对象，同时它也可以发出数据对象。因此 Flink 在内部需要对这些数据对象进行一些处理，即出于网络传输，读写状态、检查点和保存点等目的，需要对它们进行序列化和反序列化。为了提高上述过程的效率，Flink 有必要详细了解应用处理的数据类型。Flink 利用类型信息的概念来表示数据类型，并且对于每种类型，都会为其生成特定的序列化器、反序列化器以及比较器。

此外，Flink 中还有一个类型提取系统，它可以通过分析函数的输入、输出类型来自动获取类型信息，继而得到相应的序列化器和反序列化器。但在某些情况下，例如使用了 Lambda 函数或泛型类型，则必须显式指定类型信息才能启动应用或提高其性能。

## 支持的数据类型

Flink 支持 Java 和 Scala 中所有常用数据类型。例如：

- 原始类型
- Java 和 Scala 元祖
- Scala 样例类
- POJO
- 一些特殊类型

那些无法特别处理的类型会被当做泛型类型交给 Kryo 序列化框架进行序列化

### 原始类型

Flink 支持所有 Java 和 Scala 的原始类型，例如 Int、String 和 Double。

#### Java 原始类型

```java
DataStreamSource<Long> stream = env.fromElements(1L, 2L, 3L, 4L);  
stream.map(n -> n + 1).print();
```

#### Scala 原始数据

```scala
val numbers: DataStream[Long] = env.fromElements(1L, 2L, 3L, 4L)
numbers.map(n => n + 1)
```

在这里是一个 Long 类型的 DataStream，其中的元素分别为 1L，2L，3L 和 4L，对其进行 map 操作

### Java 和 Scala 元祖

元祖是由固定数量的强类型字段所组成的复合数据类型。

#### Scala 元祖

```scala
    val persons: DataStream[(String, Int)] = env.fromElements(
      ("Adam", 17),
      ("Sarah", 23)
    )
    persons.filter(p => p._2 > 18)
    persons.print()
    env.execute("Tuple")
```

#### Java 元祖

和 `Scala` 不同的是，`Java` 元组是可变的，因此可以为其字段重新赋值。函数中可以对 `Java` 元组进行重用，以减轻垃圾回收器的压力。下面的例子展示了如何对 `Java` 元组的字段进行使用和更新：

```java
// Java元祖的使用  
Tuple2<String, Integer> personTuple = Tuple2.of("Alex", 42);  // 创建一个Tuple2类型的Java元祖
Integer age = personTuple.getField(1); // 获取第2个元素，也就是age=42  
personTuple.f1 = 43; // 将第二个字段设为42  
personTuple.setField(42, 1); // 将第二个字段设为42
```

学会使用 `Java` 元祖之后，就可以使用其来创建数据流了。

```java
// 2. Java和Scala元祖  
DataStreamSource<Tuple2<String, Integer>> personStream = env.fromElements(  
        Tuple2.of("Adam", 17),  
        Tuple2.of("Sarah", 23)  
);  
// 过滤出那些年龄大于18的人  
personStream.filter(p -> p.f1 > 18);
```

可以看到，`Java` 中需要使用 `Tuple2.of()` 的方式生成元祖，`Scala` 可以直接使用 `()`，Flink 提供了 Java 元组的高效实现，它最多可包含 25 个字段，每个字段长度都对应一个单独的实现类— `Tuplel`, `Tuple2`, 直到 `Tuple25` 这些元组类都是强类型的。

### Scala 样例类

在这里定义一个样例类 Person，有两个字段：name 和 age。

```scala
    case class Person(name: String, age: Int)
    // Scala样例类
    val personsss: DataStream[Person] = env.fromElements(
      Person("Adam", 17),
      Person("Sarah", 23)
    )
    // 过滤出那些年龄大于18的人
    personsss.filter( p => p.age > 18)
    personsss.print()
    env.execute("Scala case class")
```

### POJO

Flink 会分析那些不属于任何一类的数据类型并尝试将它们作为 POJO 类型进行处理。如果一个类满足以下条件，Flink 就会将它看做 POJO：

- 是一个公有类
- 有一个共有的无参默认构造函数
- 所有字段都是公有的或提供了相应的 getter 和 setter 方法。
- 所有字段类型都必须是 Flink 所支持的

例如这样的一个类：

```java
public class Person_POJO {
    public String name;
    public int age;

    public Person_POJO(){}

    public Person_POJO(String name, int age) {
        this.name = name;
        this.age = age;
    }
}
```

这样的类 Flink 就可以进行处理：

#### Scala 处理

```scala
    // POJO
    val personssss: DataStream[Person_POJO] = env.fromElements(
      new Person_POJO("Alex", 42),
      new Person_POJO("Wendy", 23)
    )
    personssss.filter( p => p.age > 18)
    personsss.print()
    env.execute("POJO")
```

#### Java 处理

```java
// 3. POJO  
class Person {  
    // 两个字段都是公有字段  
    public String name;  
    public int age;  
  
    // 提供了默认构造函数  
    public Person(){}  
  
    public Person(String name, int age) {  
        this.name = name;  
        this.age = age;  
    }  
}  
  
env.fromElements(  
        new Person("Alex", 42),  
        new Person("Wendy", 23)  
);
```

## 为数据类型创建类型信息

`Flink` 类型系统的核心类是 `TypeInformation`，它为系统生成序列化器和比较器提供了必要信息。

例如：如果需要通过某个键值进行 `Join` 或者分组，`TypeInformation` 允许 `Flink` 对能否使用某字段作为键值进行语义检测。

需要使用 `TypeInformation` 的情况：
在类型提取器失灵的时候可能需要定义自己的类型并告知 `Flink` 如何高效地处理它们。这种情况下就需要为特定数据类型生成 `TypeInformation`。

`Flink` 为 `Java` 和 `Scala` 提供了两个辅助类，其中的静态方法可以用来生成 `Typeinformation`。

### 生成 TypeInformation

#### Java

`Java` 中的这个辅助类是 `org.apache.flink.api.common.typeinfo.Type`，具体使用方式如下：

```java
    // TypeInformation
    // 原始类型的TypeInformation
    TypeInformation<Integer> intType = Types.INT;
    // Java元祖的TypeInformation
    TypeInformation<Tuple2<Long,String>> tupleType = Types.TUPLE(Types.LONG, Types.STRING);
    // POJO的TypeInformation
    TypeInformation<Person_POJO> personType = Types.POJO(Person_POJO.class);
```

#### Scala

`Scala` `API` 中有关 `Typeinformation` 的辅助类是 `org.apache.flink.api.scala.typeutils.Types`, 它的用法如下：

```scala
    // TypeInformation
    // 1. 原始类型的TypeInformation
    val stringType: TypeInformation[String] = Types.STRING
    // 2. Scala元祖的TypeInformation
    val tupleType: TypeInformation[(Int, Long)] = Types.TUPLE[(Int, Long)]
    // 3. 样例类的TypeInformation
    val caseClassType: TypeInformation[Person] = Types.CASE_CLASS[Person]
```

## 显式提供类型信息

在 `Flink` 无法自动推断类型时需要向 `Flink` 显式提供 `TypeInformation` 对象。
上面创建的类型信息就是为了给返回值显式明确类型信息。


使用方式如下：

```java
public class ReturnTypeTest {  
    public static void main(String[] args) throws Exception {  
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();  
        env.setParallelism(1);  
  
        DataStreamSource<Event> clicks = env.fromElements(  
                new Event("Mary", "./home", 1000L),  
                new Event("Bob", "./cart", 2000L)  
        );  
  
        // 想要转换成二元组类型，需要进行以下处理  
        // 1) 使用显式的 ".returns(...)"        DataStream<Tuple2<String, Long>> stream3 = clicks  
                .map( event -> Tuple2.of(event.user, 1L) )  
                .returns(Types.TUPLE(Types.STRING, Types.LONG));  
        stream3.print();  
  
  
        // 2) 使用类来替代Lambda表达式  
        clicks.map(new MyTuple2Mapper())  
                .print();  
  
        // 3) 使用匿名类来代替Lambda表达式  
        clicks.map(new MapFunction<Event, Tuple2<String, Long>>() {  
            @Override  
            public Tuple2<String, Long> map(Event value) throws Exception {  
                return Tuple2.of(value.user, 1L);  
            }  
        }).print();  
  
        env.execute();  
    }  
  
    // 自定义MapFunction的实现类  
    public static class MyTuple2Mapper implements MapFunction<Event, Tuple2<String, Long>>{  
        @Override  
        public Tuple2<String, Long> map(Event value) throws Exception {  
            return Tuple2.of(value.user, 1L);  
        }  
    }  
}
```

# 五、定义键值和引用字段

## 定义

Flink 不是在输入类型中提前定义好键值，而是将键值定义为**输入数据上的函数**。

因此，没有必要为键和值再单独定义数据类型，这样可以省去大量样板代码。

## 字段位置

针对元祖数据类型，可以简单地使用元祖相应元素的字段位置来定义键值。

```scala
package com.coachhe.apitest

import org.apache.flink.api.java.tuple.Tuple
import org.apache.flink.api.scala.createTypeInformation
import org.apache.flink.streaming.api.scala.{DataStream, KeyedStream, StreamExecutionEnvironment}

object KeyValueTest {
  def main(args: Array[String]): Unit = {
    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
    val input: DataStream[(Int, String, Double)] = env.fromElements((1, "Hello", 1.1))
    val keyed: KeyedStream[(Int, String, Double), Tuple] = input.keyBy(1)
    keyed.print()
    env.execute("tuple_key")
  }
}
```

在这里，input.keyBy(1)表示根据第 1 个元素来进行分组，也就是 String 类型的元素。

此外，还可以通过多个元祖字段定义符合键值，只需将所有位置以列表形式逐一提供即可。

例如

```scala
val keyed: KeyedStream[(Int, String, Double), Tuple] = input.keyBy(1,2)
```

## 字段表达式

另一种定义键值和选择字段的方法是使用基于字符串的字段表达式。

它可用于元祖、POJO 以及样例类，同时还支持选择嵌套的字段。

### 样例类

现在有样例类 SensorReading

```scala
case class SensorReading(id:String, timestamp:Long, temperature:Double)
```

为了将传感器 ID 设为数据流的键值，我们可以把字段名称 id 传给 keyBy()函数

```scala
val sensorStream: DataStream[SensorReading] = env.fromElements(SensorReading("1", 20200225, 20.0), SensorReading("2", 20200226, 21.0))
val keyedSensors: KeyedStream[SensorReading, Tuple] = sensorStream.keyBy("id")
```

#### 注意

这里就不能用位置了，只有元祖可以用位置。

### POJO

现在有 POJO：

```scala
package com.coachhe.apitest;

import org.apache.flink.api.common.typeinfo.TypeInformation;
import org.apache.flink.api.common.typeinfo.Types;
import scala.Tuple2;

public class Person_POJO {
    public String name;
    public int age;

    public Person_POJO(){}

    public Person_POJO(String name, int age) {
        this.name = name;
        this.age = age;
    }
}
```

同样可以像样例类那样采用字段名称进行选择。

```scala
val POJOStream: DataStream[Person_POJO] = env.fromElements(new Person_POJO("zhangsan", 1), new Person_POJO("lisi", 2))
val POJOSensors: KeyedStream[Person_POJO, Tuple] = POJOStream.keyBy("age")
POJOSensors.print()
env.execute("POJO")
```

#### 注意

元祖字段的引用既可以使用字段名称，也可以使用从 0 开始的字段索引。不过 Scala 元祖编号从 1 开始，Java 元祖编号从 0 开始。

## 嵌套字段

如需选择 POJO 和元祖中嵌套字段，可以利用”.“来区分嵌套级别。假设有以下样例类：

```scala
case class Address(
                  address: String,
                  zip: String,
                  country: String
                  )
case class Person(
                 name: String,
                 birthday: (Int, Int, Int), //年月日
                 address: Address
                 )
```

可以看到，Person 的 address 是 Address 类型，有三种属性，我们需要通过 zip 来进行区分时可以采用下面方式：

```scala
package com.coachhe.apitest
import org.apache.flink.api.java.tuple.Tuple
import org.apache.flink.api.scala.createTypeInformation
import org.apache.flink.streaming.api.scala.{DataStream, KeyedStream, StreamExecutionEnvironment}

case class Address(
                  address: String,
                  zip: String,
                  country: String
                  )
case class Person(
                 name: String,
                 birthday: (Int, Int, Int), //年月日
                 address: Address
                 )

object KeyValueTest {
  def main(args: Array[String]): Unit = {
    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
    val person: DataStream[Person] = env.fromElements(new Person("zhangsan", (1, 1, 1), new Address("a","b","c")))
    val persons: DataStream[Person] = person.keyBy("address.zip")
    persons.print()
    env.execute(".")
  }
}
```

### 混合类型上的嵌套表达式

若需要访问嵌套在 POJO 中某一元祖的字段：

```scala
persons.keyBy("birthday._1")
```

可以使用通配符字段表达式”\_“选择全部字段

```scala
persons.keyBy("birthday._")
```

## 键值选择器

第三种指定键值的方法是使用 KeySelector 函数，它可以从输入事件中提取键值

```scala
package com.coachhe.apitest

import org.apache.flink.api.java.tuple.Tuple
import org.apache.flink.api.scala.createTypeInformation
import org.apache.flink.streaming.api.scala.{DataStream, KeyedStream, StreamExecutionEnvironment}

object KeyValueTest {
  def main(args: Array[String]): Unit = {
    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment

    val sensorStream: DataStream[SensorReading] = env.fromElements(SensorReading("1", 20200225, 20.0), SensorReading("2", 20200226, 21.0))
    // 键值选择器
    val byId: KeyedStream[SensorReading, String] = sensorStream
      .keyBy(r => r.id)
  }
}
```

KeySelector 函数接收一个输入项，返回一个键值。会根据返回的键值进行分组。

这个实例中，KeySelector 函数会返回元祖中最大的字段作为键值

```scala
package com.coachhe.apitest

import org.apache.flink.api.java.tuple.Tuple
import org.apache.flink.api.scala.createTypeInformation
import org.apache.flink.streaming.api.scala.{DataStream, KeyedStream, StreamExecutionEnvironment}

object KeyValueTest {
  def main(args: Array[String]): Unit = {
    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment

    // 键值选择器2
    val key_input: DataStream[(Int, Int)] = env.fromElements((1, 1), (2, 3))
    val keyedStream = key_input.keyBy(value => math.max(value._1, value._2))
    keyedStream.print()
    env.execute("key_selector")
  }
}
```
