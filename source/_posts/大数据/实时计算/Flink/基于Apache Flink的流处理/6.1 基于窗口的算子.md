---
title: 6.1 基于窗口的算子  
date: 2022-12-04 02:13:25  
tags: []  
categories:
  - 大数据
  - 实时计算
  - Flink
  - 基于Apache Flink的流处理
---
# 主要内容

- window 概念
- window 类型
- window API

# window 概念

streaming 流式计算是一种被设计用于处理无限数据集的数据处理引擎，而无限数据集是指一种不断增长的本质上无限的数据集， 而 window 是一种切割无限数据 为有限块进行处理的手段。

Window 是无限数据流处理的核心，Window 将一个无限的 stream 拆分成有限大小的"buckets"桶， 我们可以在这些桶上做计算操作。

# window 类型

- 时间窗口（Time Window）

  - 滚动时间窗口
  - 滑动时间窗口
  - 会话窗口

- 计数窗口（Count Window）

  基于数量的窗口具有不确定性，因为其需要依赖于元素的到达顺序。

  - 滚动计数窗口
  - 滑动计数窗口

# window API

可以用 `.window()` 来定义一个窗口，然后基于这个 window 去做一些聚合或者其他处理操作。

Flink 内置窗口分配器所创建的窗口类型为 TimeWindow。该窗口类型实际上表示两个时间戳之间的时间区间（左闭右开）。它对外提供了获取窗口边界、检查窗口是否想交以及合并重叠窗口等方法。

### 注意

window()方法必须在 keyBy 之后才能用

## 滚动窗口

<img src="https://coachhe.oss-cn-shenzhen.aliyuncs.com/Scala/20210226015355.png" style="zoom: 50%;" />

如图所示，滚动窗口分配器将元素放入大小固定且互不重叠的窗口中。

### 开启窗口方法

#### window

```scala
.window(TumblingEventTimeWindows.of(Time.seconds(1)))
```

#### timeWindow

```scala
.timeWindow(Time.seconds(1))
```

在这里滚动窗口默认和纪元时间 1970-01-01-00：00：00.000 对齐

### 指定偏移量

通过第二个参数指定一个偏移量

```scala
.window(TumblingEventTimeWindows.of(Time.seconds(1), Time.minutes(15)))
```

上述代码展示了偏移量

#### 示例

```scala
val resultStream2: WindowedStream[(String, Double), Tuple, TimeWindow] = sensorData
  .map(data => (data.id, data.temperature))
  // 用二元祖的第一个元素来进行分区
  //      .keyBy(_._1)
  .keyBy(1) //两种都可以
  // .window(TumblingEventTimeWindows.of(Time.seconds(1)))的简写
  // .timeWindow(Time.seconds(1))
  .window(TumblingEventTimeWindows.of(Time.seconds(1), Time.minutes(15)))
```

## 滑动窗口

<img src="https://coachhe.oss-cn-shenzhen.aliyuncs.com/Scala/20210226015520.png" style="zoom:50%;" />

如图所示，滑动窗口分配器将元素置于大小固定且可能重叠的窗口中。

与滚动窗口类似，用的是 SlidingEventTimeWindows

### 开启滑动窗口

```scala
// 方法1
.window(SlidingEventTimeWindows.of(Time.hours(1), Time.minutes(15)))
// 方法2
.timeWindow(Time.hours(1), Time.minutes(15))
```

类似的，可以用第三个参数可以设置偏移时间

### 设置偏移时间

```scala
.window(SlidingEventTimeWindows.of(Time.hours(1), Time.minutes(15), Time.seconds(5)))
```

## 会话窗口

<img src="https://coachhe.oss-cn-shenzhen.aliyuncs.com/Scala/20210226020035.png" style="zoom:50%;" />

由一系列事件组合一个指定时间长度的 timeout 间隙组成， 类似于 web 应用的 session， 也就是一段时间没有接收到新数据就会生成新的窗口。

特点：时间无对齐。

session 窗口分配器通过 session 活动来对元素进行分组， session 窗口跟滚动窗口和滑动窗口相比，不会有重叠和固定的开始时间和结束时间的情况，相反，当它在一个固定的时间周期内不再收到元素， 即非活动间隔产生， 那个这个窗口就会关闭。一个 session 窗口通过一个 session 间隔来配置，这个 session 间隔定义了非活跃周期的长度，当这个非活跃周期产生，那么当前的 session 将关闭并且后续的元素将被分配到新的 session 窗口中去。

```scala
.window(ProcessingTimeSessionWindows.withGap(Time.minutes(15)))
```

## 总结

若需要指定偏移量则不能用简写。

简写形式全是用来创建没有偏移量的滑动窗口和滚动窗口。

会话窗口的创建没有简写。

# 窗口函数

## DataStream 转换图

<img src="https://coachhe.oss-cn-shenzhen.aliyuncs.com/Scala/20210226163820.png" style="zoom:50%;" />

经过一系列的操作，最终 DataStream 会回到本身的类型

从 WindowStream 到 DataStream 的步骤为 window function（窗口函数）。

## 窗口函数的类型

1. 增量聚合函数。

   它的应用场景是窗口内以状态形式存储某个值且需要根语个加入窗口的元素对该值进行更新。此类函数通常会十分节省空间且最终会将聚合值作为单个结果发送出去。

   ReduceFunction 和 AggregateFunction 就属于增量聚合函数。

   也就是说，**每条数据**到来就进行计算，保持一个简单的状态。

2. 全量窗口函数

   它会收集窗口内的**所有元素**，并在执行计算时对它们进行遍历。虽然全量窗口函数通常需要占用更多空间，但它和增量聚合函数相比，支持更复杂的逻辑。

   ProcesswindowFunction 就是一个全量窗口函数。

## ReduceFunction

ReduceFunction 接收两个同类型的值并将它们组合生成一个类型不变的值。当被用在窗口化数据流上时，ReduceFunction 会对分配给窗口的元素进行增量聚合。

窗口只需要存储当前聚合结果，一个和 ReduceFunction 的输入及输出类型都相同的值。每当收到一个新元素，算子都会以该元素和从窗口状态取出的当前聚合值为参数调用 ReduceFunction，随后会用 ReduceFunction 的结果替换窗口状态。

### 优点

只需为每个窗口维护一个常数级别的小状态。此外函数的接口也很简单。

### 缺点

ReduceFunction 的应用场景存在一定局限，由于输入输出类型必须一致，所以通常仅限于一些简单的聚合。

### 每一步对应关系

<img src="https://coachhe-1305181419.cos.ap-guangzhou.myqcloud.com/%E7%A8%8B%E5%BA%8F%E5%91%98/20210609004754.png" style="zoom:50%;" />

<img src="https://coachhe.oss-cn-shenzhen.aliyuncs.com/Scala/20210227035639.png" style="zoom: 50%;" />

可以看到，先通过 map()将每个 SensorReading 类型的 DataStream 对应变换为二元祖类型的 DataStream。

然后根据 keyBy()将二元祖根据第一个元素（String）进行分区，得到 KeyedStream，其第一个元素是二元祖，第二个元素是 String。请注意，这里的 String 就是分区的键值，如果是按照第二个元素（Double）进行分区，那么得到的 KeyedStream 的第二个元素就是 Double。

然后根据 timeWindow()将 KeyedStream 变为由三个元祖组成的 WindowedStream。

最后根据 reduce()将 WindowedStream 转换为 DataStream。

在最后的

```scala
.reduce((r1, r2) => (r1._1, r1_2.min(r2._2)))
```

其中的 r1 和 r2 分别是新老数据，实时进行更新。

## AggregateFunction

### 接口

```java
@PublicEvolving
public interface AggregateFunction<IN, ACC, OUT> extends Function, Serializable {
    ACC createAccumulator();

    ACC add(IN var1, ACC var2);

    OUT getResult(ACC var1);

    ACC merge(ACC var1, ACC var2);
}
```

该接口定义了输入类型 IN，累加器类型 ACC 以及结果类型 OUT。

它和 ReduceFunction 不同的是中间数据类型以及结果类型**不再依赖输入类型**。

### 示例

使用 AggregateFunction 计算每个窗口内传感器读书的平均温度。累加器负责维护不断变化的温度总和及数量，getResult()方法用来计算平均值。

```scala
package com.coachhe.apitest

import org.apache.flink.api.common.functions.AggregateFunction
import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.api.windowing.time.Time

object AggregateFunctionTest {
  def main(args: Array[String]): Unit = {
    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
    val sensorData: DataStream[SensorReading] =
      env.fromElements(SensorReading("sensor_1", 100, 20), SensorReading("sensor_2", 101, 22))

    val avgTempPerWindow: DataStream[(String, Double)] = sensorData
      .map(r => (r.id, r.temperature))
      .keyBy(_._1)
      .timeWindow(Time.seconds(15))
      .aggregate(new AvgTempFunction)

    avgTempPerWindow.print()
    env.execute("Aggregation Test")

  }
}

class AvgTempFunction extends AggregateFunction[(String, Double), (String, Double, Int), (String, Double)] {
  override def createAccumulator(): (String, Double, Int) = {
    ("", 0.0, 0)
  }

  override def add(in: (String, Double), acc: (String, Double, Int)): (String, Double, Int) = {
    (in._1, in._2 + acc._2, 1 + acc._3)
  }

  override def getResult(acc: (String, Double, Int)): (String, Double) = {
    (acc._1, acc._2 /  acc._3)
  }

  override def merge(acc: (String, Double, Int), acc1: (String, Double, Int)): (String, Double, Int) = {
    (acc._1, acc._2 + acc1._2, acc._3 + acc1._3)
  }
}
```

## ProcessWindowFunction

### reduce()和 aggregate()的不足

它们都是对分配到窗口的事件进行增量计算。然而有时候我们需要访问窗口内的所有元素来执行一些更复杂的计算，例如计算窗口内的中值或出现频率最高的值。

### ProcessWindowFunction 接口

```scala
//
// Source code recreated from a .class file by IntelliJ IDEA
// (powered by FernFlower decompiler)
//
package org.apache.flink.streaming.api.functions.windowing;
@PublicEvolving
public abstract class ProcessWindowFunction<IN, OUT, KEY, W extends Window> extends AbstractRichFunction {
    private static final long serialVersionUID = 1L;
    public ProcessWindowFunction() {
    }
    public abstract void process(KEY var1, ProcessWindowFunction<IN, OUT, KEY, W>.Context var2, Iterable<IN> var3, Collector<OUT> var4) throws Exception;
  	// 在窗口清除时删除自定义的单个窗口状态
    public void clear(ProcessWindowFunction<IN, OUT, KEY, W>.Context context) throws Exception {
    }
  	// 保存窗口元数据的上下文
    public abstract class Context implements Serializable {
        public Context() {
        }
				// 返回窗口的元数据
        public abstract W window();
				// 返回当前处理时间
        public abstract long currentProcessingTime();
				// 返回当前事件时间水位线
        public abstract long currentWatermark();
				// 返回单个窗口状态的访问器
        public abstract KeyedStateStore windowState();
				// 用于每个键值全局状态的访问器
        public abstract KeyedStateStore globalState();
				// 向OutputTag标识的副输出发送记录
        public abstract <X> void output(OutputTag<X> var1, X var2);
    }
}
```

#### process()方法在被调用时的传入数据：

1. 窗口的键值

2. 一个用于访问窗口内元素的 Iterator

   相当于窗口内的所有元素放在一个 Iterator 里面

3. 一个用于发出结果的 Collector

4. 一个 Context 参数

   Context 对象作用（就是上面的 Context 类）

   1. 返回窗口的元数据
   2. 返回当前处理时间
   3. 返回当前事件时间水位线
   4. 返回单个窗口状态的访问器
   5. 用于每个键值全局状态的访问器
   6. 向 OutputTag 标识的副输出发送记录

### 实例

```scala
package com.coachhe.apitest


import org.apache.flink.api.scala._
import org.apache.flink.streaming.api.scala.function.ProcessWindowFunction
import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}
import org.apache.flink.streaming.api.windowing.time.Time
import org.apache.flink.streaming.api.windowing.windows.TimeWindow
import org.apache.flink.util.Collector

case class MinMaxTemp(id:String, min:Double, max:Double, endTs:Long)

/**
 * 该ProcessWindowFunction用于计算每个
 * 窗口内的最低和最高温度读数，
 * 它会将读数连同窗口结束时间戳一起发起。
 */
class HighAndLowTempProcessFunction
	extends ProcessWindowFunction[SensorReading, MinMaxTemp , String, TimeWindow]{
  override def process(key: String,
                       ctx: Context,
                       vals: Iterable[SensorReading],
                       out: Collector[MinMaxTemp]): Unit = {
    // 通过map将Iterator里面的所有数据转换成Double
    val temps: Iterable[Double] = vals.map(_.temperature)
    // 通过上下文的window得到window的结束位置
    val windowEnd: Long = ctx.window.getEnd
    out.collect(MinMaxTemp(key, temps.min, temps.max, windowEnd))
  }
}

object WindowFunctions {
  def main(args: Array[String]): Unit = {

    // set up the streaming execution environment
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    // checkpoint every 10 seconds
    env.getCheckpointConfig.setCheckpointInterval(10 * 1000)

    val sensorData: DataStream[SensorReading] = {
      env.fromElements(SensorReading("sensor1", 100, 20), SensorReading("sensor2", 101, 21))
    }
    // output the lowest and highest temperature reading every 5 seconds
    val minMaxTempPerWindow: DataStream[MinMaxTemp] = sensorData
      .keyBy(_.id)
      .timeWindow(Time.seconds(5))
      .process(new HighAndLowTempProcessFunction)

    // print result stream
    minMaxTempPerWindow.print()

    env.execute()
  }
}

```

## 增量聚合与 ProcessWindowFunction

ProcessWindowFunction 是一个功能十分强大的窗口函数。

如果在增量聚合表示逻辑之后还需要访问窗口元数据，则可以用 ReduceFunction 或 Aggregate 与功能更强的 ProcessWindowFunction 组合使用。即可以对分配给窗口的元素立即执行聚合，随后当窗口触发器触发时，再将聚合后的结果传给 ProcessWindowFunction。

### 特点

原始的 process()方法中的 Iterable 参数内有窗口的所有值。而增量聚合之后的 Iterable 参数内将只有一个值，也就是增量聚合的结果值。

### 示例

```scala
class AssignWindowEndProcessFunction
  extends ProcessWindowFunction[(String, Double, Double), MinMaxTemp, String, TimeWindow] {

  override def process(
                        key: String,
                        ctx: Context,
                        minMaxIt: Iterable[(String, Double, Double)],
                        out: Collector[MinMaxTemp]): Unit = {

    val minMax = minMaxIt.head
    val windowEnd = ctx.window.getEnd
    out.collect(MinMaxTemp(key, minMax._2, minMax._3, windowEnd))
  }
}
```

增量聚合之后的 process()

```scala
val minMaxTempPerWindow2: DataStream[MinMaxTemp] = sensorData
  .map(r => (r.id, r.temperature, r.temperature))
  .keyBy(_._1)
  .timeWindow(Time.seconds(5))
  .reduce(
    // incrementally compute min and max temperature
    (r1: (String, Double, Double), r2: (String, Double, Double)) => {
      (r1._1, r1._2.min(r2._2), r1._3.max(r2._3))
    },
    // finalize result in ProcessWindowFunction
    new AssignWindowEndProcessFunction()
  )
```

可以看到，就是在 reduce 方法之后用到了 ProcessWindowFunction。

## 自定义窗口算子

### 元素进入窗口算子之后的执行流程

1. 当一个元素进入窗口算子时会被移交给 WindowAssigner.该分配器决定了元素应该被放入哪（几）个窗口中。如果目标窗口不存在，则会创建它。
2. 如果为窗口算子配置的是增量聚合函数（如 ReduceFunction 或 AggregateFunction),那么新加入的元素会立即执行聚合，其结果会作为窗口内容存储。如果窗口算子没有配置增量聚合函数，那么新加入的元素会附加到一个用于存储所有窗口分配元素的 ListState 上。
3. 每个元素在加入窗口后还会被传递至该窗口的触发器。触发器定义了窗口何时准备好执行计算（触发），何时需要清除自身及保存的内容。触发器可以根据已分配的元素或注册的计时器（类似处理函数）来决定在某些特定时刻执行计算或清除窗口中的内容。触发器成功触发后的行为取决于窗口算子所配置的函数。如果算子只是配置一个增量聚合函数，就会发出当前聚合结果。

### 窗口分配器（WindowAssigner）

WindowAssigner 用于决定将到来的元素分配给哪些窗口。每个元素可以被加到零个、一个或者多个窗口中。

#### WindowAssigner 接口

```scala
//
// Source code recreated from a .class file by IntelliJ IDEA
// (powered by FernFlower decompiler)
//

package org.apache.flink.streaming.api.windowing.assigners;

import java.io.Serializable;
import java.util.Collection;
import org.apache.flink.annotation.PublicEvolving;
import org.apache.flink.api.common.ExecutionConfig;
import org.apache.flink.api.common.typeutils.TypeSerializer;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.windowing.triggers.Trigger;
import org.apache.flink.streaming.api.windowing.windows.Window;

@PublicEvolving
public abstract class WindowAssigner<T, W extends Window> implements Serializable {
    private static final long serialVersionUID = 1L;

    public WindowAssigner() {
    }
		// 返回元素分配的目标窗口集合
    public abstract Collection<W> assignWindows(T var1, long var2, WindowAssigner.WindowAssignerContext var4);
		// 返回WindowAssigner的默认触发器
    public abstract Trigger<T, W> getDefaultTrigger(StreamExecutionEnvironment var1);
		// 返回WIndowAssigner中窗口的TypeSerializer
    public abstract TypeSerializer<W> getWindowSerializer(ExecutionConfig var1);
		// 表明此分配器是否创建基于事件时间的窗口
    public abstract boolean isEventTime();
		// 用于访问当前处理时间的上下文
    public abstract static class WindowAssignerContext {
        public WindowAssignerContext() {
        }
				// 返回当前处理时间
        public abstract long getCurrentProcessingTime();
    }
}

```

##### 示例

用于滚动事件时间窗口的窗口分配器

```scala
package com.coachhe.apitest

import java.util
import java.util.Collections

import org.apache.flink.api.common.ExecutionConfig
import org.apache.flink.api.common.typeutils.TypeSerializer
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment
import org.apache.flink.streaming.api.windowing.assigners.WindowAssigner
import org.apache.flink.streaming.api.windowing.triggers.{EventTimeTrigger, Trigger}
import org.apache.flink.streaming.api.windowing.windows.TimeWindow

object DIY_WindowFunction {
  def main(args: Array[String]): Unit = {

  }

}

class MyWindowAssigner extends WindowAssigner[Object, TimeWindow] {

  val windowSize: Long = 30 * 1000L

  override def assignWindows(
                              o: Object,
                              ts: Long,
                              ctx: WindowAssigner.WindowAssignerContext): java.util.List[TimeWindow] =
    {
      // 30s取余
      val startTime: Long = ts - (ts % windowSize)
      val endTime: Long = startTime + windowSize
      // 发出相应的时间窗口
      Collections.singletonList(new TimeWindow(startTime, endTime))
    }

  override def getDefaultTrigger(
                                  env: StreamExecutionEnvironment): Trigger[Object, TimeWindow] = {
    EventTimeTrigger.create()
  }

  override def getWindowSerializer(
                                    executionConfig: ExecutionConfig):
  TypeSerializer[TimeWindow] = {
    new TimeWindow.Serializer
  }

  override def isEventTime: Boolean = true
}
```

### 触发器（Trigger）

触发器用来定义何时对窗口进行计算并发出结果。它的触发条件可以是时间，也可以是某些特定的数据条件。

触发器不仅能够访问时间属性和计时器，还可以使用状态，因此它在某种意义上等价于处理函数。

每次调用触发器都会生成一个 TriggerResult，它用于决定窗口接下来的行为。

TriggerResult 可以是以下值之一：

1. CONTINUE

   什么都不做

2. FIRE

   如果窗口算子配备了 ProcessWindowFunction 则会调用该函数并发出结果。如果只有一个增量函数，那么直接发出聚合结果，窗口状态不会有任何变化。

3. PURGE

   完全清除窗口内容

4. FIRE_AND_PURGE

   先进行窗口的计算（FIRE），随后删除所有状态及元数据（PURGE）。

### 移除器（Evictor）

Evictor 是 Flink 窗口机制中的一个可选组件，可用于在窗口执行计算前或后从窗口删除元素。

移除器常用语 GlobalWindow，它支持清理部分窗口内容而不必完全清除整个窗口状态。
