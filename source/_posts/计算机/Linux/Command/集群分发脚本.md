# scp

## scp定义

scp可以实现服务器与服务器之间的数据拷贝（from server1 to server2） 

## 基本语法 

```shell
scp -r $pdir/$fname $user@hadoop$host:$pdir/$fname 
```

### 解释

<img src="https://coachhe.oss-cn-shenzhen.aliyuncs.com/Docker/20210407090837.png" style="zoom:50%;" />

## 常见用法 

1. 本机数据拷贝到别的服务器 
2. 从别的服务器将数据拷贝到本机 
3. 将某台非本机的服务器的数据拷贝到另一台主机 

### 案例1 

在hadoop100上，将hadoop100中/home/hadoop_learning目录下的软件拷贝到hadoop102上。 

```shell
scp -r hadoop_learning/ root@hadoop101:/home/hadoop_learning
```

 ![](https://coachhe.oss-cn-shenzhen.aliyuncs.com/Docker/20210407090953.png)

结果：

<img src="https://coachhe.oss-cn-shenzhen.aliyuncs.com/Docker/20210407091011.png" style="zoom:50%;" />

注意：

<img src="https://coachhe.oss-cn-shenzhen.aliyuncs.com/Docker/20210407091033.png" style="zoom:50%;" />

### 案例2

在hadoop102上从hadoop100处拉取/home/hadoop_learning下的数据 

```shell
sudo scp -r coachhe@hadoop100:/home/hadoop_learning /home/hadoop_learning
```

![](https://coachhe.oss-cn-shenzhen.aliyuncs.com/Docker/20210407091130.png)

结果：

<img src="https://coachhe.oss-cn-shenzhen.aliyuncs.com/Docker/20210407091202.png" style="zoom:50%;" />

### 案例3

在hadoop101上将hadoop100的数据拷贝到hadoop102 

```shell
scp -r coachhe@hadoop100:/home/hadoop_learning root@hadoop102:/home/hadoop_learning2
```

 ![](https://coachhe.oss-cn-shenzhen.aliyuncs.com/Docker/20210407091243.png)

# 远程同步工具rsync

## rsync和scp的区别

### 优点 

rsync主要用于备份和镜像。具有速度快，避免复制相同内容和支持符号链接的优点。 

### 和scp的区别 

用rsync做文件的复制要比scp速度快，rsync只对差异文件做更新，scp是把所有文件都复制过去。 

## 语法

```shell
rsync -rvl $pdir/$fname $user@hadoop$host:$host:$pdir/$fname
```

### 选项说明 

| 选项 | 功能         |
| ---- | ------------ |
| -r   | 递归         |
| -v   | 显示复制过程 |
| -l   | 拷贝符号连接 |

## 案例

将hadoop100机器上的/home/newdisk目录同步到hadoop101服务器root目录下的/home/newdisk下。 

```shell
sudo rsync -rvl /home/newdisk/ root@hadoop101:/home/newdisk 
```

![](https://coachhe.oss-cn-shenzhen.aliyuncs.com/Docker/20210407091442.png)

# 编写集群分发脚本xsync

## 代码 

```shell
#!/bin/bash 

#1 获取输入参数个数，如果没有参数则直接退出 
pcount=$# 
if((pcount==0)); then 
	echo no args; 
	exit; 
fi 

#2 获取文件名称 
p1=$1 
fname=`basename $p1` #这里就是拿到最终文件的名称，/opt/module就是module 
echo fname=$fname 

#3 获取上级目录到绝对路径 
pdir=`cd -P $(dirname $p1); pwd` #这里是得到文件的路径 
echo pdir=$pdir 

#4 获取当前用户名称 
user=`whoami` #也就是coachhe 

#5 循环 
for((host=101;host<103;host++)); do 
	echo -------- hadoop$host --------- 
	rsync -rvl $pdir/$fname root@hadoop$host:$pdir 
done 
```

## 脚本执行

![](https://coachhe.oss-cn-shenzhen.aliyuncs.com/Docker/20210407091638.png)

### 执行结果

<img src="https://coachhe.oss-cn-shenzhen.aliyuncs.com/Docker/20210407091655.png" style="zoom:50%;" />









